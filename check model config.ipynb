{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b52bdb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, BeitForImageClassification, Swinv2Config, Swinv2Model, SwiftFormerForImageClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e276ea75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5fe7dee6b34471a9a3d78fda68520f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "vit = AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224',\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        id2label=dict(), label2id=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f020b416",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_name_or_path\": \"google/vit-base-patch16-224\",\n",
       "  \"architectures\": [\n",
       "    \"ViTForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {},\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {},\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.34.1\"\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03ea19f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/deit-base-patch16-224 were not used when initializing ViTForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "deit = AutoModelForImageClassification.from_pretrained('facebook/deit-base-patch16-224',\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        id2label=dict(), label2id=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "742726db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTConfig {\n",
       "  \"_name_or_path\": \"facebook/deit-base-patch16-224\",\n",
       "  \"architectures\": [\n",
       "    \"ViTForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"encoder_stride\": 16,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {},\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {},\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"model_type\": \"vit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"patch_size\": 16,\n",
       "  \"qkv_bias\": true,\n",
       "  \"transformers_version\": \"4.34.1\"\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deit.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11dad001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "178b1356ad17471d9dd15676d2ce00d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d51788cfab4eb99c9bbfc3e7631d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/414M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/beit-base-patch16-224-pt22k-ft22k were not used when initializing BeitForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing BeitForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BeitForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "beit = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k',\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        id2label=dict(), label2id=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0832f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeitConfig {\n",
       "  \"_name_or_path\": \"microsoft/beit-base-patch16-224-pt22k-ft22k\",\n",
       "  \"architectures\": [\n",
       "    \"BeitForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"auxiliary_channels\": 256,\n",
       "  \"auxiliary_concat_input\": false,\n",
       "  \"auxiliary_loss_weight\": 0.4,\n",
       "  \"auxiliary_num_convs\": 1,\n",
       "  \"drop_path_rate\": 0.1,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 768,\n",
       "  \"id2label\": {},\n",
       "  \"image_size\": 224,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 3072,\n",
       "  \"label2id\": {},\n",
       "  \"layer_norm_eps\": 1e-12,\n",
       "  \"layer_scale_init_value\": 0.1,\n",
       "  \"model_type\": \"beit\",\n",
       "  \"num_attention_heads\": 12,\n",
       "  \"num_channels\": 3,\n",
       "  \"num_hidden_layers\": 12,\n",
       "  \"out_indices\": [\n",
       "    3,\n",
       "    5,\n",
       "    7,\n",
       "    11\n",
       "  ],\n",
       "  \"patch_size\": 16,\n",
       "  \"pool_scales\": [\n",
       "    1,\n",
       "    2,\n",
       "    3,\n",
       "    6\n",
       "  ],\n",
       "  \"semantic_loss_ignore_index\": 255,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_absolute_position_embeddings\": false,\n",
       "  \"use_auxiliary_head\": true,\n",
       "  \"use_mask_token\": false,\n",
       "  \"use_mean_pooling\": true,\n",
       "  \"use_relative_position_bias\": true,\n",
       "  \"use_shared_relative_position_bias\": false,\n",
       "  \"vocab_size\": 8192\n",
       "}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beit.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fdbe7022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecfe4f04fab4f5d9cdebeff45d0ce5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/69.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8225a5339ad7425bad8b3e59c5feaab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading pytorch_model.bin:   0%|          | 0.00/352M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/swinv2-base-patch4-window16-256 were not used when initializing Swinv2ForImageClassification: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing Swinv2ForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Swinv2ForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "swin = AutoModelForImageClassification.from_pretrained('microsoft/swinv2-base-patch4-window16-256',\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        id2label=dict(), label2id=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ca62894",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swinv2Config {\n",
       "  \"_name_or_path\": \"microsoft/swinv2-base-patch4-window16-256\",\n",
       "  \"architectures\": [\n",
       "    \"Swinv2ForImageClassification\"\n",
       "  ],\n",
       "  \"attention_probs_dropout_prob\": 0.0,\n",
       "  \"depths\": [\n",
       "    2,\n",
       "    2,\n",
       "    18,\n",
       "    2\n",
       "  ],\n",
       "  \"drop_path_rate\": 0.1,\n",
       "  \"embed_dim\": 128,\n",
       "  \"encoder_stride\": 32,\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"hidden_dropout_prob\": 0.0,\n",
       "  \"hidden_size\": 1024,\n",
       "  \"id2label\": {},\n",
       "  \"image_size\": 256,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"label2id\": {},\n",
       "  \"layer_norm_eps\": 1e-05,\n",
       "  \"mlp_ratio\": 4.0,\n",
       "  \"model_type\": \"swinv2\",\n",
       "  \"num_channels\": 3,\n",
       "  \"num_heads\": [\n",
       "    4,\n",
       "    8,\n",
       "    16,\n",
       "    32\n",
       "  ],\n",
       "  \"num_layers\": 4,\n",
       "  \"patch_size\": 4,\n",
       "  \"path_norm\": true,\n",
       "  \"pretrained_window_sizes\": [\n",
       "    0,\n",
       "    0,\n",
       "    0,\n",
       "    0\n",
       "  ],\n",
       "  \"qkv_bias\": true,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_absolute_embeddings\": false,\n",
       "  \"window_size\": 16\n",
       "}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swin.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "684ceee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at MBZUAI/swiftformer-l3 were not used when initializing SwiftFormerForImageClassification: ['head.weight', 'dist_head.bias', 'head.bias', 'dist_head.weight']\n",
      "- This IS expected if you are initializing SwiftFormerForImageClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing SwiftFormerForImageClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "swift = SwiftFormerForImageClassification.from_pretrained('MBZUAI/swiftformer-l3',\n",
    "                                                        ignore_mismatched_sizes=True,\n",
    "                                                        id2label=dict(), label2id=dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f5fe7274",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SwiftFormerConfig {\n",
       "  \"_name_or_path\": \"MBZUAI/swiftformer-l3\",\n",
       "  \"architectures\": [\n",
       "    \"SwiftFormerForImageClassification\"\n",
       "  ],\n",
       "  \"batch_norm_eps\": 1e-05,\n",
       "  \"depths\": [\n",
       "    4,\n",
       "    4,\n",
       "    12,\n",
       "    6\n",
       "  ],\n",
       "  \"down_pad\": 1,\n",
       "  \"down_patch_size\": 3,\n",
       "  \"down_stride\": 2,\n",
       "  \"downsamples\": [\n",
       "    true,\n",
       "    true,\n",
       "    true,\n",
       "    true\n",
       "  ],\n",
       "  \"drop_path_rate\": 0.0,\n",
       "  \"embed_dims\": [\n",
       "    64,\n",
       "    128,\n",
       "    320,\n",
       "    512\n",
       "  ],\n",
       "  \"hidden_act\": \"gelu\",\n",
       "  \"id2label\": {},\n",
       "  \"label2id\": {},\n",
       "  \"layer_scale_init_value\": 1e-05,\n",
       "  \"mlp_ratio\": 4,\n",
       "  \"model_type\": \"swiftformer\",\n",
       "  \"num_channels\": 3,\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.34.1\",\n",
       "  \"use_layer_scale\": true\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swift.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebdf97d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
